\chapter{Machine Learning}%
\label{ch:ml}

The next chapter is somewhat of a diversion from the physics discussed so far.
It will focus on techniques in machine learning which are often referred to in
High Energy Physics as a Multi-variate Analysis (MVA). The reason for this
diversion is that these techniques have become very widespread in the field,
they are used in several of the reconstruction and selection algorithms that are
used to obtain the events on which the analysis is performed. Furthermore an MVA
is used to obtain the distribution that acts as the final discriminant for the
analysis, and machine learning techniques are also used to model the
backgrounds. Given how widespread the use of these techniques is in the analysis
it makes sense to describe them before diving into the details.

The two main algorithms used are Boosted Decision Trees (BDTs) and Neural
Networks (NNs) which will be described in sections~\ref{sec:bdts}
and~\ref{sec:neural-networks} respectively. These algorithms are used in many
places outside High Energy Physics and so rather than referring to individual
pieces of data that enter into the algorithm as an event, in this chapter they
will be referred to as an example. This terminology comes from the fact that in
general these algorithms must be shown a large number of examples before they
are suitable ``trained'' for their purpose, and that in general those examples
could be data that represent anything. Both of these algorithms can be operated
in classification or regression modes. The main difference between these modes
is that classification mode provides a score for each of a given number of
classes which can be interpreted as a probability that a given example belongs
to the given class, whereas regression outputs a single number per example whose
interpretation depends on the problem.

Both algorithms can be written as a function of some inputs $\vec{x}$, some
weights to be found during training $\vec{w}$, and a set of hyper-parameters
$\vec{\theta}$ as follows
\begin{equation}
  F(\vec{x}, \vec{w}, \vec{\theta} \,) = \vec{y},
  \label{eq:ml-general}
\end{equation}
where $\vec{y}$ is a vector whose outputs correspond to each class in a
classification problem. If the algorithm is set up for a regression problem then
the output will just be one number. The hyper-parameters control the behaviour
of the algorithm in question and are set by hand in advance of training.
Training either algorithm involves an iterative process where at each iteration
the function is evaluated for the current set of weights, the outputs are
compared against some truth labels $\vec{t}$ via the computation of a loss
function. Based on the value of the loss function the weights are then updated
according to the given algorithm. It is therefore vital that firstly these truth
labels are available for the data on which one wants to train (e.g. train on
simulated predictions rather than real data) and secondly that the examples are
split into a training and testing set so that the set which is used to
iteratively update weights is not the same set that performance is evaluated on.
This avoids over-fitting to any noise in a given set, though this problem is not
circumvented entirely and over-fitting will be addressed for specific algorithms
in the sections ahead.

\section{Boosted Decision Trees}%
\label{sec:bdts}
We will start by discussing a decision tree for a classification problem.
Decision trees have a structure as in figure~\ref{fig:bdt}, which shows a tree
dividing examples into two classes, red and blue.
\input{04-machine-learning/bdt}
Each circular node in the tree represents a cut on one of a number of variables
provided as input to the algorithm. The tree is read top to bottom with each
node being followed by two edges branching left and right that represent the
path taken by examples which pass or fail the cut respectively. Square nodes
represent that the termination criteria have been reached and that events in
these nodes have been classified according to the colour of the node. The
variable chosen at each node is optimised in order to maximise a criteria
related to the separation of classes.  For a problem containing two classes a
common separation criteria is the Gini index,
\begin{equation}
  G = p(1-p),
  \label{eq:gini}
\end{equation}
where p is the purity of a chosen class that one wants to maximise.

A decision tree by itself is able to separate examples into a number of classes,
however a single tree is prone to over--training. Over--training is the term
used to describe the phenomena where a classifier learns from statistical
fluctuations in the data rather learning a generalised separation boundary
between classes. The reason that decision trees are susceptible to this is that
if two variables yield a similar separation criteria then a fluctuation in the
training data may lead to the choice of one variable over another for a
particular node, this choice will lead to a very different tree structure than
if the fluctuation were not present.

In order to mitigate the over--training tendencies of decision trees they are
often used in an ensemble algorithm such as bagging~\cite{bagging}
or boosting~\cite{boosting}. Here only boosting will be discussed. Ensembles of
decision trees are often referred to as random forests. Boosting works by
training a sequence of trees, weighting misclassified events from a tree so that
they have more influence over the structure of the next tree in the sequence.
The final classification of any given example is a weighted average over all
trees, this can be weighted by the overall accuracy of each tree, but in general
can take any weighted average.

\subsection{Gradient Boosting}

Gradient boosting is the name of an optimisation algorithm that takes the
concept of boosting and combines it with the gradient descent algorithm. A
pictographic representation of gradient descent for a regression problem can be
seen in figure~\ref{fig:grad-desc.}
\input{04-machine-learning/grad-desc}
The basic principle of gradient descent is to find the minimum of some
multi-variable function by taking the derivative of the function around some
starting point in the space and moving in the direction of the negative
gradient. This process is repeated iteratively until some termination criteria
is reached. In machine learning gradient descent is most often applied to the
loss function which describes the discrepancy between the predictions of a model
$\vec{y}$ and the truth labels $\vec{t}$. For a loss written as
\begin{equation}
  \mathcal{L}(\vec{y}, \vec{t} \,),
\end{equation}
the predictions and the labels define a point in the space around which the
function must be locally differentiable, it is hard to determine where these
points will be before training an algorithm on a given dataset and so in general
a loss is chosen that is differentiable everywhere.

Considering now a boosted decision tree that has a number of iterations for each
of which a decision tree is constructed using the Gini index. The model can be
written as
\begin{equation}
  F_i(\vec{x}),
\end{equation}
at a given iteration and is evolved by the addition of a single decision tree as
\begin{equation}
  F_i({\vec{x}}) + d(\vec{x}) = F_{i+1}(\vec{x}),
\end{equation}
the decision tree which is added is found by taking the negative gradient of the
loss computed at the previous iteration
\begin{equation}
  d{\vec{x}} = - \frac{\partial \mathcal{L}(F_i, \vec{t} \,)}{\partial F}.
\end{equation}
This is process is known as gradient boosting. Each decision tree is known as a
weak learner, by evolving the overall model by stepping in the direction of the
negative gradient of the loss calculated on the predictions of the model at the
current stage the algorithm aims to correct for past mistakes.

The aforementioned hyper-parameters that control the properties of the model
come in a few forms for the BDT. They include but are not limited to, the
maximum tree depth and the number of weak learners to train. These hyper
parameters change the structure of the model in a way that would be obvious when
drawing a visual representation. The choice of boosting algorithm can also be
considered a hyper-parameter, in this work gradient boosting is chosen in
general which is a change from the previously popular AdaBoost~\cite{AdaBoost}. 

\section{Neural Networks}%

\label{sec:neural-networks}

Like boosted decision trees neural networks can be described as a function of
input data, weights and hyper-parameters $F(\vec{x}, \vec{w}, \vec{\theta} \,)$.
Unlike BDTs neural networks can vary a lot more in their structure, the
hyper-parameters of these algorithms allow for much finer control over the
behaviour of the function as an estimator. The basic structure of a neural
network is shown in figure~\ref{fig:nn}.  The building blocks of the NN resemble
Fisher discriminants~\cite{Fisher} and take the form
\begin{equation}
a_j = \sum_{i=1}^{d} w_{ji}x_{i} + w_{j0},
\label{eq:fisher}
\end{equation}
where the $w_{ji}$ terms are known as weights and the $w_{j0}$ as biases, these
constructions are called activations. It would be remiss of me to cite the works
of Fisher without condemning his participation in the field of eugenics, but I
leave the citation here as a reminder that as scientists we have a
responsibility to society to pursue causes for good. Neural network models are
inspired by neurons in the brain, who fire when some threshold of
neuro--transmitting chemical is reached~\cite{biology} and are linked up in
intricate ways to manifest complex behaviours. In order to take activations and
give them a behaviour similar to neurons they must be passed through an
activation function, denoted $\mathcal{H}$, which gives the threshold effect
\begin{equation}
h_j = \mathcal{H}(a_j).
\label{eq:hiddenunit}
\end{equation}
These are similar to Rosenblatt's original perceptron~\cite{Rosenblatt} with the
difference that $\mathcal{H}$ must be differentiable whereas the perceptron used
a step function.

All neural networks are made of layers, and have an input layer which is a
vector of input data and an output layer, a vector whose size relates to the
predictions being made. Layers inbetween these two are called hidden layers and
are made of the units $h_j$ (known as hidden units). The most simple neural
network has a single hidden layer. This model can be written as
\begin{equation}
F(\vec{x},\vec{w}, \vec{\theta}) = \mathcal{O} \Bigg( \sum_{j=1}^{m} w_{kj}^{(2)}
\mathcal{H} \Bigg( \sum_{i=1}^{d} w_{ji}^{(1)} x_{i} + w_{j0}^{(1)} \Bigg) + w_{k0}^{(2)} \Bigg).
\label{eq:basicnn}
\end{equation}
where the superscript number in brackets labels the layer to which parameters
belong. The hyper-parameters $\vec{\theta}$ here are the choice of activation
function $\mathcal{H}$, output function $\mathcal{O}$, number of hidden units
$m$ and the number of hidden layers (just one here). A depiction of a single
layered NN can is shown in figure~\ref{fig:single-nn}.
\input{04-machine-learning/nn}

The output function $\mathcal{O}$ must have properties that allow us to
interpret the outputs of the network as probabilistic. A common choice of output
function for classification problems is the softmax function
\begin{equation}
  \mathcal{O}(z)_k = p(k|\vec{x}) = \frac{exp(z_k)}{\sum_{i=1}^kexp(z_i)}
  \label{eq:softmax}
\end{equation}
where $z$ merely denotes the argument of the output function. This function
gives the probability of being an example belonging to class $k$ given the data
$\vec{x}$ where there are $k$ possible classes.

By controlling the hyper-parameters a network of any size and shape can be
built, in the network function can be written in general as the cumbersome
\begin{equation}
  F(\vec{x}, \vec{w}, \vec{\theta}) = \mathcal{O} \Bigg( \sum_{j_{n}=0}^{m_{n}} w_{kj_{n}}
  \mathcal{H}_{n} \Bigg( \dots \mathcal{H}_2  \Bigg( \sum_{j_{1}=0}^{m_{1}} w_{j_{2}j_{1}} 
  \mathcal{H}_{1} \Bigg( \sum_{i=0}^{d} w_{j_{1}i} x_{i} \Bigg) \Bigg) \dots \Bigg) \Bigg).
  \label{eq:fullnn}
\end{equation}
This behaves in a similar way to the network with one hidden layer but now each
layer's width and activation must be chosen individually, though often they are
set to the same values. A diagram of an arbitrary sized neural network is shown
in figure~\ref{fig:full-nn}.
\input{04-machine-learning/nn}
It is clear that the number of parameters that need to be learned $\vec{w}$ and
the number that need to be set by hand to tune the model to a given problem
$\vec{theta}$ are much larger than in the BDT algorithm. For this reason
adoption of neural networks is sometimes hampered by lack of data or sufficient
time to tune the hyper-parameters to achieve satisfactory performance.

Loss functions:
\begin{equation}
E(\vec{w}) = \frac{1}{2}\sum_{n=1}^{N}(y(\vec{x}_n, \vec{w}) - t_n)^2
\label{eq:sumofsquares}
\end{equation}
where $t_n$ are the targets for the given data entries $\vec{x}_n$.
Minimising this function with some algorithm does work in practice, however it
has been shown that, using
\begin{equation}
E(\vec{w}) = - \sum_{n=1}^{N} \Bigg (t_n \ln (y_n) + (1-t_n) \ln (1-y_n) \Bigg)
\label{eq:xentropy}
\end{equation}
known as cross-entropy, is faster and more generalised \cite{XEntropySimard}
(further discussion on generalisation in section \ref{sec:generalisation}). The
particular training algorithm that will be used to update parameters in this

Optimisers:
report is known as adaptive moment estimation or ADAM \cite{ADAMOpt}. ADAM is a
variant of the gradient descent algorithm, which is widely used and has spawned
many other variants \cite{GDOverview}. The reasons for picking between ADAM and
vanilla gradient descent are given in chapter \ref{ch:netarch}.



