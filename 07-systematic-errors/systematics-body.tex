\chapter{Systematic Uncertainties}%
\label{ch:systematics}

Systematic uncertainties considered in the analysis come in one of two forms,
either a shape effect or a normalisation effect. Normalisation effects alter the
number of events in a given sample across the entire sample simply changing the
total number of events. Shape effects change where events lie in a given
distribution causing events to migrate between bins of a histogram, and
potentially across boundaries that are used to define analysis regions described
in~\ref{sec:ana-regions}. Each systematic uncertainty is controlled by a
nuisance parameter in the profile-likelihood fit. For all Gaussian penalty terms
the a prior uncertainty is estimated in advance.

A sub-category of normalisation effect is the acceptance effect which deals with
the normalisation in a particular region or set of regions. The purpose of the
acceptance uncertainties is to account for any mismodelling in the theoretical
prediction of the quantities used to categorise events into regions be it the
leptonic channel, jet multiplicity or an analysis region. The priors of these so
called acceptance uncertainties are calculated using the double ratio, unless
stated otherwise,
\begin{equation}
  \left. \dfrac{N_{A}^{\text{nominal}}}{N_{B}^{\text{nominal}}} \middle/
    \dfrac{N_{A}^{\text{alternative}}}{N_{B}^{\text{alternative}}} \right.,
  \label{eq:acceptance-dr}
\end{equation}
where the formula has been written agnostic of the specific application so
$N_{A}^{\text{nominal}}$ is the number of  events in the nominal prediction in
some region or category called $A$ and so on.

Another variant of the normalisation effect are the flavour composition
uncertainties. Rather than impacting the normalisation of an entire background
in terms of how they are categorised in the analysis plots, for example in
section~\ref{sec:prefit}, they impact the sub-processes split by flavour of the
decay products individually. A heavy flavour process is defined as one where the
two leading jets have flavour $(b,~b)$, $(b,~c)$, $(b,~l)$, or $(c,~c)$,
this categorisation is often written as hf for short for example when
considering all heavy flavour sub-process of the $V+$jets background one could
write $V+$hf. Flavour composition uncertainties affect the normalisation of one
of the flavour sub-processes with respect to one of the others, specific details
will follow under the sections relating to the relevant backgrounds.

The rest of this chapter will detail the sources of systematic error broken down
into groups of similar origin. Before breaking down each group of systematic
uncertainties, a technique called the Boosted Decision Tree Re-weighting (BDTr)
method will be explained as it used across a number of the different groups. The
author's contributions include the determination of all of the $Z+$jets
systematic uncertainties, determination of systematic uncertainties relating to
the flavour composition of simulated top process events, and development and
testing of the BDTr method.

\section{Parametrising Variance Due To Shape Uncertainties}
\label{sec:re-weighting}

The predictions entering into the profile-likelihood fit of the analysis can be
written as a probability density function (PDF) $p(\vec{x}|\vec{\theta})$,
where $\vec{x} = [x_{1},..., x_{i}]$ is a vector of observable quantities, with
$N$ elements and $\vec{\theta}$ represents the theoretical parameters of the
model. The parameters may come from the Standard Model theory or
phenomenological considerations that must be taken into account to turn that
theory into a usable prediction.

As described above, shape effects may impact the prediction of a given
Monte-Carlo generator that has a particular set of parameters. The choice of
generator will change the prediction due to different choices made by the
generator creators, including but not limited to, the parton shower model,
hadronisation model and non-perturbative processes. One way to get a handle on
the variance of a particular generator is therefore to vary these
choices by picking an alternative generator and drawing a comparison to the
alternative prediction. This method is flawed in ways which will be discussed
but is nonetheless fairly common and often one of the few choices available to
analysers when they are trying to get an idea of what the systematic
uncertainties on the modelling of complex physics processes are. Two such
methods of comparison will be detailed here, the single dimension
parameterisation described in section~\ref{sec:1D-reweight} and a
multi-dimensional parameterisation described in section~\ref{sec:ND-reweight}.
Following this a hybrid approach which uses both methods will be described in
section~\ref{sec:hybrid-reweight}.

\subsection{Single Dimension Parameterisation}
\label{sec:1D-reweight}

The single dimension parameterisation uses the ratio of probability densities
\begin{equation}
  r(\vec{x}) = \frac{p(\vec{x}|\vec{\theta}_{1})}{p(\vec{x}|\vec{\theta}_{0})},
  \label{eq:DensityRatio}
\end{equation}
where the subscripts on $\vec{\theta}$ number different choices of the
parameters which govern the model, for example due to different choice of
generator. Here the subscript 0 denotes the nominal prediction and 1 denotes an
alternative. In order to map the nominal to the alternative it is clear that the
one can multiply $p(\vec{x}|\vec{\theta}_{0})$ by the ratio.

The only way that this calculation is tractable is to consider a single
dimension of the probability density function, specifically a single observable
chosen from the vector $\vec{x}$. The ratio then becomes
\begin{equation}
  r(x_{i}) = \frac{p(x_{i}|\vec{\theta}_{1})}{p(x_{i}|\vec{\theta}_{0})},
  \label{eq:1D-ratio}
\end{equation}
which can in turn be calculated for as many or as few as the elements of
$\vec{x}$ as is desired. When the ratio is calculated using only a single
variable it can still be used to map the $N$ dimensional nominal prediction to
the alternative, however the mapping is only guaranteed to be successful in the
observable $x_i$ chosen to calculate the ratio. In general agreement between the two
predictions in the other variables can not be relied upon.

In practice the ratio is used to derive a systematic shape uncertainty by
weighting each event in the nominal prediction as a function of the variable
$x_{i}$ whose distribution was used to calculate the ratio. Limitations on the
sample size of each prediction mean that in practice it is better to smooth the
ratio via the use of a parametric fit in order to mitigate any large statistical
fluctuations in a single bin of the ratio.

\subsection{$N$-dimensional Parametrisation}
\label{sec:ND-reweight}

As already alluded to in the previous section there are issues with the
parametrisation in a single dimension. Not only is the mapping only guaranteed
to work in a single dimension but if the technique is applied sequentially on
one variable after another the mapping from the first step can be undone by the
second. That is to say that if one calculates the ratio in
equation~\ref{eq:1D-ratio} with $i=1$ and $i=2$ the re-weighting by $r(x_2)$
will not preserve the agreement between the two predictions that would be
achieved by the re-weighting with $r(x_1)$. It is clear that this is because a
ratio for a given $x_i$ only encode information relating to the PDFs
$p(x_i|\theta_0)$ and $p(x_i|\theta_1)$ whereas an encoding of the multi-variate
distributions $p(\vec{x}|\theta_0)$ and $p(\vec{x}|\theta_1)$ is desired.

A function capable of providing a univariate representation of the full
multi-variate distribution has already been discussed, these are the functions
that one obtains when training a machine learning classifier such as those
mentioned in chapter~\ref{ch:ml}. Functions of the form given in
equation~\ref{eq:ml-general} can encode information from the multi-variate input
$\vec{x}$ into a lower dimensional $\vec{y}$ that captures the correlations
between the different elements of the input $\vec{x}$. Specifically for this
parametrisation the model is trained to classify events as coming from either
the Monte-Carlo generator with parameters $\theta_0$ or $\theta_1$, the output
is therefore written as $\vec{y} = [y_0, y_1]$ where each term in the vector
represents the probability that an event comes from $p(\vec{x} | \theta_0)$ or
$p(\vec{x} | \theta_1)$ respectively. Note that the choice of output function in
the model must ensure that probability is conserved, i.e. $y_0 + y_1 = 1$. This
technique has been demonstrated in the
literature~\cite{cranmer2016approximating} where the mathematical details are
discussed in more detail.

Our setup allows us to create the following approximation
\begin{equation}
  r(\vec{x}) =  \frac{p(\vec{x}|\vec{\theta}_{1})}{p(\vec{x}|\vec{\theta}_{0})}
  \approx \frac{F(\vec{x}, \vec{w})[1]}{F(\vec{x}, \vec{w})[0]},
  \label{eq:bdtr-approximation}
\end{equation}
where $F$ is our trained model whose hyper-parameters we can considered fixed
and drop from the notation (previously denoted $\vec{\theta}$ in
chapter~\ref{ch:ml}). The numbers in the square brackets indicate which element
of the $\vec{y}$ is being selected. Once the above approximation is made, the
steps to generate weights and perform a parameterisation of the shape
discrepancy between two predictions is the same as in the one dimensional case.
By placing certain restrictions on $F$ one can turn the approximation into an
equivalence, this is discussed in~\cite{VHModellingNote2019}, however for what
is considered here the approximation will be the focus. As mentioned this
parametrisation will be referred to in the analysis as the BDTr method.

\subsection{Hybrid $\mathbf{(N - 1)}$--Dimensional Parametrisation}
\label{sec:hybrid-reweight}

It is possible to combine the two aforementioned parametrisation strategies into
a single strategy. This is achieved by first re-weighting using the
1--dimensional technique and then training the classifier on the re-weighted
distributions and proceeding with the $N$--dimensional parametrisation as usual.
In principle the 1--dimensional re-weighting can be performed a number of times
sequentially before training the classifier and so therefore a general
$(N-k)$--dimensional strategy can be formed, however in this analysis the
1--dimensional re-weighting is only applied once before training.

The hybrid approach yields two parametrisations one parametrising the
difference between the two samples in the single variable used in the
1--dimensional re-weighting and a second that parametrises the remaining
variables using the density ratio formed by using the classifier trained on the
re-weighted distributions. This approach will be referred to as the factorised
BDTr method as the single variable that is not included in the classifier that
is used to generate the multi-dimensional parametrisation is considered to be
factored out.

There are a number of reasons why one might want to factor out a single variable
from the multi-dimensional procedure that naively looks superior in every way to
the 1--dimensional approach. Internal parameters of two predictions models need
not obey a one-to-one mapping, therefore a smooth interpolation between them is
not guaranteed to exist. This reveals an incongruity between the nature of the
Gaussian penalty term used to control the shape uncertainty in the fit, which is
varied smoothly and continuously, and the underlying mapping that it aims to
characterise.  A second nuisance parameter arising from the factorised variable
therefore at least allows the profile-likelihood fit to control in a more
well defined way the histograms relating to that variable. The inclusion of a
factorised variable may also be desirable as particular variables have increased
importance to the analysis, for example the histograms entering in the control
regions are of $p_{\mathrm{T}}^V$ and so giving the fit a parameter to control
directly the shape of this variable is considered to aid interpretability.

\section{Experimental Systematic Uncertainties}
\label{sec:experimental-systs}

This section describes experimental systematic uncertainties, these arise due to
limitations of the hardware discussed in chapter~\ref{ch:detector} and
reconstruction algorithms described in chapter~\ref{ch:recon}. All of these
uncertainties are provided by different combined performance (CP) groups and
made centrally available to members of the ATLAS collaboration, this ensures a
consistent understanding of the performance of the detector and the
reconstruction algorithms of Athena. A summary of all of the experimental
systematic uncertainties used in the analysis can be found in
table~\ref{tab:expSyst}.
\clearpage
\newpage
\input{07-systematic-errors/experimental-table.tex}

\subsection{Luminosity and Pile-up}
\label{sec:lumisys}

As discussed in chapter~\ref{ch:detector} luminosity is used to measure how much
data is recorded by the detector in any span of time. The uncertainty is
calculated for each year of running and is determined to be  2.1\%, 2.6\%,
2.4\%, and 2.0\% for the years 2015, 2016, 2017, 2018 respectively. A combined
uncertainty is calculated for the entire period 2015--2018 at 1.7\%. The
methodology used to calculate this figure is similar to that detailed
in~\cite{lumiDetermine}, from calibrations of the luminosity scale using x--y
beam-separation scans~\cite{lumiTwiki}.

Pile-up uncertainties are computed by changing the nominal data scale of
$1.0/1.03$ to $1.0/1.00$ and $1.0/1.18$ to get the up and down variations
respectively~\cite{puTWiki}.

\subsection{Triggers}
\label{sec:trigsys}

\subsubsection{\texorpdfstring{$E_{\mathrm{T}}^{\text{miss}}$}{MET} Triggers}

Scale factors are derived for the $E_{\mathrm{T}}^{\text{miss}}$ triggers using
$W(\mu,\nu)+$jets events as outlined in~\cite{VHObjectNote2019}. Three
uncertainties are taken into account, the statistical error on the dataset used
to derived the scale factor \texttt{METTrigStat}, a parameter used to account
for the choice of physics process and the effect that might have on the
determination of the scale factor \texttt{METTrigTop} and \texttt{METTrigSumPt}
which aims to account for dependence of the offline $S_{\mathrm{T}}$ (as defined
in~\ref{sec:0lep-selection}) on the trigger efficiency. The uncertainty
\texttt{METTrigTop} is named as such because it is derived from a comparison of
the scale factors as calculated with a $t\bar{t}$ sample and compared to the
nominal sample. The uncertainty relating to $S_{\mathrm{T}}$ is only applied to
events recorded in 2017 due to a specific trigger used in this year.

\subsubsection{Lepton trigger}

\begin{sloppypar}
  The nuisance parameter \texttt{EL\_EFF\_Trigger\_Total\_1NPCOR\_PLUS\_UNCOR} is
  used to control the overall uncertainty on the electron trigger. For the muons,
  the two components are considered \texttt{MUON\_EFF\_TrigSystUncertainty} and
  \texttt{MUON\_EFF\_TrigStatUncertainty}, which account for the systematic error
  and the statistical error on scale factor respectively. An up and down variation
  of 1-$\sigma$ are used for all of the aforementioned nuisance parameters.
\end{sloppypar}

\subsection{Electrons}

\subsubsection{Electron Efficiency Uncertainties}

The efficiency of the electron reconstruction and identification has a
systematic uncertainty called
\texttt{ElectronEfficiencyCorrection}~\cite{electronTWiki} calculated using the
full run 2 data. Reconstruction is 97-99\% efficient across the full
$p_{\mathrm{T}}$~spectrum. Identification has an efficiency scale factor
available from $p_{\mathrm{T}}>7$~\GeV. An isolation efficiency scale factor is
also included. The latest uncertainties that are available include scale factors
for $p_{\mathrm{T}}>150$~\GeV\ that are unity due to a lack of enough data to
measure a scale factor. An additional systematic uncertainty of $\pm 2\%$ is
assigned above 150~\GeV\ to mitigate this shortcoming. The above uncertainties
are controlled by \texttt{EL\_EFF\_ID\_Total\_1NPCOR\_PLUS\_UNCOR},
\texttt{EL\_EFF\_Reco\_Total\_1NPCOR\_PLUS\_UNCOR}, and
\texttt{EL\_EFF\_Iso\_Total\_1NPCOR\_PLUS\_UNCOR}.

\subsubsection{Electron Energy Scale and Resolution Uncertainties}

A large number of electron energy scale and resolution systematic uncertainties
are available~\cite{EgammaCalibTWiki}. The analysis is not very sensitive to
these quantities and therefore only two uncertainties are considered which are
called \texttt{EG\_RESOLUTION\_ALL} and \texttt{EG\_SCALE\_ALL}.

\subsection{Muons}

\subsubsection{Muon Efficiency Systematic Uncertainties}

Samples of $Z\to\mu\mu$ and $J/\psi\to\mu\mu$ events from the full 2015
dataset (corresponding to 3.2~fb$^{-1}$) are used to calculate scale factors to
account for uncertainties in the reconstruction, isolation and track-to-vertex
association~\cite{muonTWiki}. These scale factors are valid in the full
$p_{\mathrm{T}}$ spectrum with the $J/\psi$ measurement providing more accurate
determination in the $p_{\mathrm{T}}$<15~\GeV\ region and the $Z$ measurement
being more accurate in the $p_{\mathrm{T}}$>15~\GeV\ region. Four independent
systematic uncertainties are considered which are called
\texttt{MUON\_EFF\_RECO\_STAT}, \texttt{MUON\_EFF\_RECO\_STAT\_LOWPT},
\texttt{MUON\_EFF\_RECO\_SYS}, \texttt{MUON\_EFF\_RECO\_SYS\_LOWPT}, which are
split based on the $p_{\mathrm{T}}$ measurement. Statistical and systematic
uncertainties on the isolation scale factor are controlled by
\texttt{MUON\_EFF\_ISO\_STAT} and \texttt{MUON\_EFF\_ISO\_SYS} respectively.
They are supported in the range of $10 < p_{\mathrm{T}} < 500$~\GeV. For muons
outside of this range, a scale factor of 1$\pm$0.05 is used.
\texttt{MUON\_EFF\_TTVA\_STAT} and \texttt{MUON\_EFF\_TTVA\_SYS} control the
systematic uncertainty on the scale factor of the cuts on the impact parameter
significance and the $|z_0\sin\theta|$ which estimate the error on
track-to-vertex association. All of the above systematic uncertainties are
derived using $\pm 1\sigma$ variations in the relevant samples.

\subsubsection{Muon Momentum Scale and Resolution Uncertainties}

Muon momentum scale and resolution uncertainties that are
considered~\cite{muonTWiki} have been calibrated using a sample of  $Z\to\mu\mu$
events in the region with $p_{\mathrm{T}}$~>~20~\GeV\ and with a sample of
$J/\psi\rightarrow \mu\mu$ events in region with $p_{\mathrm{T}}$~<~20~\GeV.
Parameters exist to control uncertainties due to the inner detector, muon system
and the overall momentum scale, they are called \texttt{MUONS\_ID},
\texttt{MUONS\_MS} and \texttt{MUONS\_SCALE} respectively. These systematic
uncertainties are derived by varying the momentum scale and the track position
in the detector by $\pm 1\sigma$. Two parameters which account for the charge
dependence of the of the momentum scale uncertainty \texttt{MUON\_SAGITTA\_RHO}
and \texttt{MUON\_SAGITTA\_RESBIAS} are also included.

\subsection{Taus}

Uncertainties on the measurement of taus do not have a large effect
on the analysis. \texttt{TAUS\_TRUEHADTAU\_SME\_TES\_DETECTOR},
\texttt{TAUS\_TRUEHADTAU\_SME\_TES\_INSITU} and
\texttt{TAUS\_TRUEHADTAU\_SME\_TES\_MODEL}, are considered, which all account
for different sources of energy scale uncertainty.

\subsection{Jets}

A baseline set of parameters controlling uncertainties on jets is available
however in the analysis a set which is reduced to 23 nuisance parameters using
principal component analysis~\cite{PCA} is used. The baseline set accounts for
effects due to eta calibration, high-$p_{\mathrm{T}}$ jets, pile-up, flavour
composition, flavour response, $b$-jets, and punch-through jets. The 23 nuisance
parameters and a short description of each are displayed in
table~\ref{tab:expSyst} under the category jets and $b$-tagging where all of the
relevant nuisance parameters start with \texttt{JET} and
\texttt{JET\_CR\_Flavour\_Composition} represents three independent nuisance
parameters as explained in its short description. The largest source of
uncertainty amongst the chosen set come from the jet energy scale and the jet
energy resolution. The determination of the former is documented
in~\cite{JetCalibration2015} and the latter is determined from data versus
Monte-Carlo prediction comparisons.

\subsection{$E_{\mathrm{T}}^{\text{miss}}$}

Systematic uncertainties considered account for calorimeter and track based
jets, and can be found under the heading $E_{\mathrm{T}}^{\text{miss}}$ in
table~\ref{tab:expSyst}.

\subsection{Flavour Tagging}

Uncertainties relating to flavour tagging are expected to have a large impact on
the analysis. Systematic uncertainties on the tagging are implemented as scale
factors. The procedure is such that for each event the scale factor is applied
if it is determined to contain a $b$-jet by the tagger, otherwise the
inefficiency scale factor is applied, which yields the nominal event weight. For
each of uncertainties considered the scale factor and inefficiency scale factor
are varired and applied as above, note the varied inefficiency scale factor will
no longer yield the nominal event weight.

Similarly to in the determination of the systematic uncertainties on the jets, a
large number of individual systematics are available (about 40 per jet flavour)
and so in order to have a smaller set to work with a principle component
analysis of the full set is performed. For the working point and reduction
scheme chosen in this analysis there are 3 variations for $b$-jets, 3 variations
for $c$-jets and 5 variations for light-jets. These are shown in
table~\ref{tab:expSyst}, they all start with \texttt{FT} and end with a
descriptor of which jet flavour they apply to e.g. \texttt{B0}.

Two additional systematic uncertainties irrespective of reduction scheme are
considered that relate to the $p_{\mathrm{T}}$
extrapolation~\cite{BTaggingExtrap2015} and charm-to-bottom quark extrapolation,
they are called also listed in table~\ref{tab:expSyst}, start with \texttt{FT}
and contain the descriptor \texttt{extrapolation}.

\section{Systematic Uncertainties on $V+$jets Events}
\label{sec:vjets}

The $V+$jets processes are simulated with
\textsc{Sherpa}~2.2.1~\cite{1126-6708-2009-02-007} as mentioned in
section~\ref{sec:composition}, which is interfaced with the
NNPDFs~\cite{Ball:2012cx} for both the matrix element calculation and the parton
shower tuning. A feature of \textsc{Sherpa}~2.2.1 is used which provides a
combination of matrix elements with different parton multiplicities in order to
simulate events with many additional jets, which contribute significantly to the
background. Up to 2 extra partons are included in the next-to-leading order
matrix element, and 3 or 4 extra partons are included at leading order in QCD.
Different parton multiplicities are combined using  a matching scheme based on
the CKKW-L~\cite{Lonnblad:2001iq, Lavesson:2005xu} merging technique, with a
merging scale of $Q_{\text{cut}}=20$~\GeV\footnote{$Q_{\text{cut}}$ describes
  the parton momentum sum above which the parton showering is used and below which
  the matrix element calculation is used.}. Simulation of events with more than 4
extra partons rely on the parton shower algorithm of \textsc{Sherpa}. The parton
shower and underlying event models are included in \textsc{Sherpa} whose
generator adopts a full 5--flavour scheme with $b$- and $c$-quarks being treated
as massless in the matrix element. Massive quarks can be produced in the parton
shower and heavy flavours can be produced directly in the scattering process of
the underlying event.

The analysis gains a lot of sensitivity from high $p_{\mathrm{T}}^V$ regions
with two $b$-jets. Therefore it is desirable to have a dataset of predictions
of these events such that statistical fluctuations are smaller than in the data.
In order to achieve this samples are simulated in specific slices of
max$(p_{\mathrm{T}}^V, H_{\mathrm{T}})$, which is the larger of the
$p_{\mathrm{T}}$ of the vector boson or the $H_{\mathrm{T}}$ of the event.
The following slices are used
\begin{equation*}
  \text{max}(p_{\mathrm{T}}^V, H_{\mathrm{T}}) =
  [0\text{--}70, 70\text{--}140, 140\text{--}280,
  280\text{--}500, 500\text{--}1000, >1000]~\GeV.
\end{equation*}
Also filters are applied on the flavour of jets in the event. The filters used
are shown in table~\ref{tab:bc-filters}, and are not applied to the highest
slice in max$(p_{\mathrm{T}}^V, H_{\mathrm{T}})$.
\input{07-systematic-errors/bc-filters}
The filtering strategy for $Z\to\nu\nu$ samples differs in the mc16e campaign using
a combination of $p_{\mathrm{T}}^Z$ and $m_{jj}$ to better populate the region
above the $E_{\mathrm{T}}^{\text{miss}}$ trigger thresholds. These samples also
use a tighter $b$-filter compared to their mc16a/d counterpart which is also
described in table~\ref{tab:bc-filters}. All nominal $V+$jets samples with the
corresponding max($H_{\mathrm{T}}$,$p_{\mathrm{T}}^V$) slices and flavour
filters are listed in tables~\ref{tabular:mc_samples_Wjets}
\ref{tabular:mc_samples_Zlljets}, and
\ref{tabular:mc_samples_Zvvjets} in the appendices. A set of alternative
predictions are used for study and determination of priors of systematic
uncertainties. If a discrepancy is common to a nominal versus data and an
alternative versus data comparison then the discrepancy may arise from
experimental uncertainty rather than a shortcoming of the modelling.

The alternative samples are generated using
\textsc{MadGraph}~5~\cite{MADGRAPH5_aMC@NLO} interfaced to \textsc{Pythia}~8 for
the modelling of the parton shower and the underlying event. The
\textsc{MadGraph}~5 v2 generator provides a LO (QCD) description of these
processes, merging together matrix-element calculations with different parton
multiplicities, up to 4 additional jets, higher jet multiplicities are modelled
by the parton shower algorithm. The merging scheme is the same as for the
nominal samples, but has a merging scale of $Q_{\text{cut}} = 30$~\GeV. For the
LO ME calculation the NNPDF2.3 LO PDFs are used, with $\alpha_S = 1.3$.
Similarly to
\textsc{Sherpa}~2.2.1, also \textsc{MadGraph}~adopts a full 5-flavour scheme
with massless quarks in the ME calculation, while massive quarks can be produced
by the parton shower. All alternative $V+$jets samples are listed in
tables~\ref{tabular:zjetsAlternativeSamples} and
\ref{tabular:wjetsAlternativeSamples} in the appendices.

As well as using an alternative Monte-Carlo generator the nominal generator,
\textsc{Sherpa}, includes systematic variations internally. Every
\textsc{Sherpa}~2.2.1 $V+$jets sample has an event weight corresponding to each
of the variations detailed in table~\ref{tab:sherpa-variations}\footnote{Some of
  the variations cannot be produced by \textsc{Sherpa}~2.2.1 and so
  \textsc{Sherpa}~2.1 is used instead. For these variations half of the
  variation in each direction is taken as the uncertainty rather than comparing
  to the central value of the \textsc{Sherpa}~2.2.1 prediction.}.
\input{07-systematic-errors/vjets-sherpa-vars.tex}

\subsubsection{$V+$jets Cross Section}

The $V+$jets cross sections are known at NNLO (QCD)~\cite{Butterworth:1287902},
the higher order cross sections are used to normalise the V+jets samples in the
analysis. For the $W+$jets samples the total cross section from \textsc{Sherpa}
or from \textsc{MadGraph}, averaged across all 3 lepton flavours taking into
account the different hadron filter efficiencies, is scaled to the NNLO
prediction obtaining scaling factor of $k_{\text{NNLO}}^{\text{QCD}} = 0.9702$.

For $Z+$jets some subtleties must be considered. For $Z\to\ell\ell +$jets the
cut at generator level, $m_{\ell\ell}>40$ \GeV, must be taken into account. The
NNLO calculation uses a cut of $66<m_{\ell\ell}<116$ \GeV, which can be applied
at before reconstruction to the samples of this analysis in order to get the
scaling correct as follows
\begin{equation}
  k_{\text{NNLO}}^{\text{QCD}} =
  \frac{\sigma_{\text{NNLO}}(66<m_{\ell\ell}<116\,{\GeV})}%
  {\sigma_{\textsc{Sherpa},\textsc{MadGraph}}(66<m_{\ell\ell}<116\,{\GeV})},
  \label{eq:nnlo-vjets-k}
\end{equation}
hence the scaling factor is found to be $0.9751$. For $Z \to \nu\nu$ +jets the
NNLO no theoretical cross section is available. Values from the
Particle Data Group (PDG)~\cite{PDG} are used to correct for the difference
between the $Z \to \nu\nu$ and $Z \to \ell\ell$ branching ratios, and the NNLO
cross section is used without any mass cuts and with the $Z/\gamma^*$
interference removed. The scaling factor is therefore calculated to be $0.9728$.
The discrepancy in the factors for $Z\to\ell\ell$ and $Z\to\nu\nu$ events can be
explained by the fact the two generators use different branching ratios for
these processes compared with those used by the theoretical calculations.

\subsection{Systematic Uncertainties on $W+$jets Events}
A number of nuisance parameters are introduced to account for modelling
uncertainties on the $W+$jets background process. These uncertainties are
considered and derived in the 0-- and 1-- lepton channels only, as the amount of
$W+$jets background present in the 2--lepton channel is negligible. A summary of
all of the uncertainties for this background can be found in
table~\ref{tab:wjets_systematics}.

The nominal and alternative predictions described in~\ref{sec:vjets} are used to
in the determination of all of the values described in the following. 

\subsubsection{Normalisation and Acceptance Uncertainties}
A summary of the normalisation uncertainties is shown in
table~\ref{tab:wjetsnorm}.
\input{07-systematic-errors/wjets-norm-table}
A single nuisance parameter is introduced for each of the $W+cl$ and $W+l$
processes which are both heavily suppressed by the analysis requirement of two
b-tagged jets. There is a large mismodelling of the normalisation of the $W+$hf
process and so floating normalisations are used separately in the 2--jet and
3--jet categories. Priors for several acceptance uncertainties are calculated
using equation~\ref{eq:acceptance-dr}\footnote{Whilst the priors are obtained
  with the double ratio formula involving two regions, the effect of the
  uncertainty is applied to only one of the two regions.}, which are used to
control the migration of events between the 0-- and 1--lepton channels and the
migration between the $\Delta R(b, \bar{b})$ regions.

\begin{sloppypar}
  A parameter controlling migration between the two channels considered,
  \texttt{SysWbbNorm\_L0}, is applied to both the 2--jet and 3--jet regions of the
  0--lepton channel. The choice to apply to the 0--lepton channel and not the
  1--lepton channel is made because the 1--lepton channel provides a better
  constraint on the $W+$hf process. Whilst all alternative predictions are
  considered the size of this prior is dominated by the difference between the
  nominal prediction and \textsc{MadGraph}.
\end{sloppypar}

Migration between the $\Delta R(b, \bar{b})$ control regions and the signal
region is controlled by one parameter per control region. Named
\texttt{SysWbbCRSRextrap}, it can be seen in table~\ref{tab:wjetsnorm} that it
is applied in each of the control regions, and is also correlated across jet
multiplicity and leptonic channel. The shape uncertainty which uses the BDTr
method discussed in section~\ref{sec:ND-reweight} induces migration effects
across the analysis regions, however that systematic is taken as a shape only
effect. The migrations induced by the shape uncertainty agree well with those
derived from comparisons between nominal and alternative predictions, and the
sign of the final priors is determined by migration due to the shape
uncertainty. The magnitude of the prior is calculated as the quadrature sum of
the difference between the nominal prediction and \textsc{MadGraph} as well as
the scale and parton density function variations included in \textsc{Sherpa}.
There is no dedicated nuisance parameter to control the migration between the
$p_{\mathrm{T}}^V$ regions of the analysis as this migration is controlled by
the $p_{\mathrm{T}}^V$ shape uncertainty.

\subsubsection{Flavour Composition Uncertainties}

As introduced at the start of this chapter the $W+$jets background has a $W+$hf
component that is comprised of the $bb$, $bc$, $bl$ and $cc$ sub-components. The
fractional contribution of each of these sub-components is detailed in
table~\ref{tab:whf-comp}.
\input{07-systematic-errors/whf-components}
Flavour composition uncertainties are computed using
equation~\ref{eq:acceptance-dr} for each sub-component, the calculated priors
are summarised in tables~\ref{tab:wjets-flavour-comp} separately in the 0-- and
1--lepton channels\footnote{Whilst these numbers were calculated in this
  iteration of the analysis, incomplete availability of \textsc{Sherpa} internal
  variations lead to a set of numbers from a previous iteration being used,
  those are displayed in this work.}.
\input{07-systematic-errors/wjets-flavour-comp}
The flavour composition uncertainties are dominated by the difference between
\textsc{Sherpa} and \textsc{MadGraph}. Individual uncertainties for different
jet multiplicities or analysis regions are not considered necessary due to a
small amount of non-$bb$ events remaining after the 2 $b$-jet requirement is
imposed.

\subsubsection{Shape Uncertainties}

The hybrid $(N-1)$--dimensional parametrisation technique described
in~\ref{sec:hybrid-reweight} is used to generate uncertainties on the shape of
the distributions of the $W+$jets background. The variable that has been
factorised out is $p_{\mathrm{T}}^V$ and so there are nuisance parameters which
control the $p_{\mathrm{T}}^V$ shape in each of the 2-- and 3--jet categories as
well as a nuisance parameter that controls the multi-variate shape uncertainty
across all regions and jet multiplicities. These nuisance parameters are
summarised in table~\ref{tab:wjets-shapes}.
\input{07-systematic-errors/wjets-shapes-table}

Shapes are derived in regions split by leptonic channel and heavy flavour
sub-component, yielding eight regions total. The $W+l$ and $W+cl$ components of
the $W+$jets backgrounds are not considered as they account for less than 1~\%
of all background events in any region.

In all regions the initial 1--dimensional re-weighting is performed by comparing
the ratio of the nominal \textsc{Sherpa}~2.2.1 $(p_{\mathrm{T}}^V,
E_{\mathrm{T}}^{\text{miss}})$ prediction with that of \textsc{MadGraph}. The
requirement of $p_{\mathrm{T}}^V>75$~\GeV\ is applied to all distributions in
the 1--lepton channel and in the 0--lepton channel the requirement of
$E_{\mathrm{T}}^{\text{miss}}>150$~\GeV\ is imposed instead.
Figures~\ref{fig:wjets_1lep_2jet_SysWPtVBDTr}
and~\ref{fig:wjets_1lep_3jet_SysWPtVBDTr} show the comparison between
\textsc{Sherpa} and \textsc{MadGraph} in the  $p_{\mathrm{T}}^V$ variable in the
2-- and 3--jet categories, respectively.
\input{07-systematic-errors/wjets-1lep-ptv-shapes}
The systematic uncertainties on the shape are shown in the ratio panel at the
bottom of each plot in green.

The low number of events available in the 0--lepton channel mean that it is not
statistically viable to generate a parametrisation in that channel, therefore
the shapes derived in the 1--lepton channel are also used in the 0--lepton
channel, a comparison of the two approaches has been
studied~\cite{VHModellingNote2019}.

As per the methodology of the factorised BDTr technique the nominal
\textsc{Sherpa}~2.2.1 events are re-weighted by the $p_{\mathrm{T}}^V$ ratio
with the alternative prediction. The re-weighted \textsc{Sherpa}~2.2.1 events
are then trained against the \textsc{MadGraph} prediction in a BDT classifier
that uses the input variables detailed in table~\ref{tab:MVAinputs}. A separate
training is performed in each jet category and for each sub-component of the
$W+$hf process in congruence with the 8 regions that the $p_{\mathrm{T}}^V$
shape was derived in. Considering the output of the classifier the ratio of the
scores of the nominal and alternative predictions enter into the
profile-likelihood fit as a nuisance parameter called
\texttt{SysBDTr\_W\_SHtoMG5}. All of the following plots shown contain only
$W+bb$ events as this is the dominant and most important component of the $W+$hf
process and indeed the $W+$jets background in the analysis.

In order to check the validity of the method the re-weighted nominal predictions
of inputs to the analysis MVA are plotted against the nominal and alternative
predictions in figures~\ref{fig:wjets_1lep_2jet_BDTrClosure_1}
and~\ref{fig:wjets_0lep_2jet_BDTrClosure_1} for the 1-- and 0--lepton channels
respectively in the 2--jet category.
\input{07-systematic-errors/wjets-bdt-shapes-1lep2jet}
\input{07-systematic-errors/wjets-bdt-shapes-0lep2jet}
Should the re-weighted nominal inputs match
the shapes of the alternative inputs then the method is considered to be valid.
Indeed upon inspection one can see that this is largely the case across the
board apart from small levels of disagreement. These small levels of
disagreement are disregarded as the two most important variables in the
determination of the analysis BDT score, $m(b, \bar{b})$, $p_{\mathrm{T}}^V$ and
$E_{\mathrm{T}}^{\text{miss}}$ have good levels of agreement.

The actual effect of the factorised BDTr systematic uncertainty on the analysis
BDT score is shown in figures~\ref{fig:wjets_1lep_FullRun2MVA_BDTrClosure}
and~\ref{fig:wjets_0lep_FullRun2MVA_BDTrClosure} for the 1-- and 0--lepton
channels respectively. 
\input{07-systematic-errors/wjets-bdt-shapes-mva-1lep}
\input{07-systematic-errors/wjets-bdt-shapes-mva-0lep}
Tables~\ref{tab:wjets-extrapolation_uncertainties_pTV}
and~\ref{tab:wjets-extrapolation_uncertainties_BDTr} in the appendix show the
extrapolation uncertainties induced by the $p_{\mathrm{T}}^V$ and factorised
BDTr shape systematic uncertainties respectively.
\clearpage
\newpage

\subsection{Systematic Uncertainties on $Z+$jets Events}
\label{sec:zjets-systs}

Systematic uncertainties are considered in the 0-- and 2--lepton channels only
as there is a negligible contribution of this background in the 1--lepton channel.

\subsubsection{Normalisation and Acceptance Uncertainties}

Several normalisation and acceptance uncertainties are considered for the Z +
jets background, they are summarised in table~\ref{tab:zjetsnorm}.
\input{07-systematic-errors/zjets-norm-table}
One nuisance parameter is used for each of the $Z+l$ and $Z+cl$ components as
they are sub-dominant accounting for less than 1~\% of the total background in
any region due to the requirement of two $b$-jets, they are called
\texttt{SysZlNorm} and \texttt{SysZclNorm} respectively. These normalisations
are each correlated across all regions.

The $Z+$hf process has a large normalisation uncertainty and so separate
nuisance parameters are used for the 2--jet and 3--jet regions, these are called
\texttt{norm\_Zbb\_J2} and \texttt{norm\_Zbb\_J3} respectively. These parameters
are heavily constrained in the fit due to a very large number of $Z+$jets events
in the 0-- and 2--lepton signal regions and due to high $Z+$jets purity
particularly in the 2--lepton control regions.

Nuisance parameters are introduced to the model migration of events between
regions. The priors for these parameters are calculated using the double ratio
in equation~\ref{eq:acceptance-dr}. Firstly a parameter is introduced in order
to account for the difference in the number of $Z+$jets in the 0-- and 2--lepton
channels, is it called \texttt{SysZbbNorm\_0L} and is applied to $Z+$hf events
in the 0--lepton channel as the 2--lepton channel has a higher purity of Z +
jets events and therefore yields a better constraint on the normalisation. The
size of the prior as calculated by comparing \textsc{Sherpa}~2.2.1 and
\textsc{MadGraph} is similar to the size as calculated by examining the
\textsc{Sherpa} internal weights.

The nuisance parameter that controls the $Z+$jets $p_{\mathrm{T}}^V$
uncertainty, \texttt{SysZPTV} (see section~\ref{sec:zjets-shapes}), is also
allowed to control the relative number of events between analysis regions. This
choice is made as the magnitude of the extrapolation uncertainties shown in
table~\ref{tab:zjets-ptv-extrap} are similar in size to the largest variations
one would obtain by comparing the nominal prediction to the
alternatives.\input{07-systematic-errors/zjets-shape-yield-effect}

\begin{sloppypar}
  Induced normalisation effects by the $m_{bb}$ shape are smaller than those
  arising from comparison of the different available predictions. Therefore this
  systematic uncertainty is applied as a shape effect only and an additional
  nuisance parameter is introduced to cover the migration between analysis
  regions. As will be explained in section~\ref{sec:zjets-shapes} it is necessary
  to de-correlate the $m_{bb}$ shape systematic in the lowest $p_{\mathrm{T}}^V$
  bin of the analysis. The nuisance parameter controlling the migration between
  analysis regions in the $m_{bb}$ variable is also de-correlated in that bin.
  These acceptance uncertainties are applied to the CR$_{\text{high}}$ and
  CR$_{\text{low}}$ separately, the nuisance parameter are
  \texttt{SysZbbCRSRExtrapolation\_CRLow}, de-correlated as
  \texttt{SysZbbCRSRextrap\_BMin75\_L2\_CRLow} and
  \texttt{SysZbbCRSRExtrapolation\_CRHigh} de-correlated as
  \texttt{SysZbbCRSRextrap\_BMin75\_L2\_CRHigh}. The determination of the prior
  for these nuisance parameters is dominated by the difference between
  \textsc{Sherpa}~2.2.1 and \textsc{MadGraph}.
\end{sloppypar}

\subsubsection{Flavour Composition Uncertainties}

The $Z+$hf process breaks down in the exact same way as the $W+$hf process, into
$(b,b)$, $(b, c)$, $(b, l)$ and $(c, c)$ sub-components. Uncertainties on the
fraction that each of these makes up of the $Z+$hf process are found in
table~\ref{tab:zjets-flavour-comp}.
\input{07-systematic-errors/zjets-flavour-comp}
They are calculated using equation~\ref{eq:acceptance-dr} and applied to the
non-$(b, b)$ component of the ratio. The nuisance parameters entering into the
fit are \texttt{SysZbcZbbRatio}, \texttt{SysZccZbbRatio} and
\texttt{SysZblZbbRatio}. Priors are calculated separately in the 0-- and
2--lepton channels, and in the 2--lepton channel priors are also calculated
separately in the 2-- and 3--jet categories. The variations are nonetheless
considered to be correlated across regions. The value of the priors is
completely dominated by the difference between the nominal \textsc{Sherpa}~2.2.1
prediction and the alternative \textsc{MadGraph} prediction.

\subsubsection{Shape Uncertainties}
\label{sec:zjets-shapes}

The shape uncertainties for the $Z+$jets process are derived using the
1--dimensional parametrisation approach detailed in
section~\ref{sec:1D-reweight} with one notable difference. Instead of a ratio of
predictions generated with different parameters, the ratio in this case is of
the nominal \textsc{Sherpa}~2.2.1 prediction and the data itself. In order to be
valid this method has a number of requirements, the first of which is that the
region of interest must have a high purity of the background for which the
uncertainty is derived. In the case of the 2--lepton channel the sidebands of
the region which contains both the \VHbb\ signal and the $V\!Z\!\to\! b\bar{b}$
background, defined as  $80 \GeV < m_{bb} < 140 \GeV$, is quite pure in $Z+$jets
events. The purity can be enhanced by subtracting from the data the top quark
background events which contribute second to the $Z+$jets events, this can be
done with a high degree of certainty over the template as the data from the top
$e\mu$ CR can be used. The remaining backgrounds contribute a very small amount
to the region and so even with large uncertainties on their template the method
is still sufficiently accurate. Neither the signal or diboson background are
subtracted from the data as they have been removed by the veto of the
aforementioned range in $m_{bb}$ and the veto is applied to the selection so
that events in the range are removed from all distributions considered.

This method is used to derived shape uncertainties on the $p_{\mathrm{T}}^V$ and
$m_{bb}$ variables as they are considered to be the most important in the
analysis. The

uncertainty is used in the 0--lepton channel which adds an additional
complication. As mentioned in section~\ref{sec:2lep-selection} a kinematic fit
is used in order to correct the $b$-jet momentum and these corrected jets are then
used in the formulation of $m_{bb}$ in the 2--lepton channel. This means that
$m_{bb}$ in the 0-- and 2--lepton channels refers to a different quantity and so
a shape uncertainty derived on one may not be suitable to use on the other. In
order to get around this issue the shape uncertainty for $m_{bb}$ is derived on
a variable that is calculated using globally sequentially calibrated (GSC) jets
and is called GSC $m_{bb}$, these jets have not undergone the kinematic fit
procedure and so ought to match more closely between the two leptonic channels.
The aforementioned veto on events in the range $80 \GeV < m_{bb} < 140 \GeV$ is
therefore actually applied based on GSC $m_{bb}$.

The plots in figures~\ref{fig:zjets-mbb-shapes} and~\ref{fig:zjets-ptv-shapes}
show the nominal \textsc{Sherpa}~2.2.1 prediction of the $Z+$jets background in
the 2--lepton channel in the sum of analysis regions CR$_{\text{low}}$, SR and
CR$_{\text{high}}$ plotted against the subtracted data in the $p_{\mathrm{T}}^V$ and GSC
$m_{bb}$ variables respectively.
\input{07-systematic-errors/zjets-mbb-shape-plots}
\input{07-systematic-errors/zjets-ptv-shape-plots}
The subtracted data is the data with a template comprised of the data from the
top $e\mu$ CR and the nominal predictions of all backgrounds other than $Z+$jets
subtracted from it. Plots are shown in the 2--jet, $\geq$3--jet and $\geq$2--jet
categories (the sum of the two jet multiplicity based categories in the
analysis) and are plotted across all $p_{\mathrm{T}}^V$ bins of the analysis. The shape
uncertainty is derived by taking a fit to the ratio in the bottom pane of each
plot, the fit defines the 1-$\sigma$ up variation of the Gaussian constrained
nuisance parameter, the symmetrised version of the fit (not displayed) defines
the 1-$\sigma$ down variation. The $\geq$2--jet region is used to calculate the
prior for the systematic uncertainties \texttt{SysZPtV} and \texttt{SysZMbb}
that are applied to all $Z+$jets events in the 0-- and 2--lepton channels.

The nuisance parameters are correlated across leptonic channels, however as can
be seen by inspection of figure~\ref{fig:zjets-mbb-shapes} the fitted shape does
not cover the discrepancy between the subtracted data and the prediction at low
values of $m_{bb}$. Investigating the plots in
figure~\ref{fig:zjets-mbb-shape-ptv-range} shows that this discrepancy is coming
from the $75~\GeV < p_{\mathrm{T}}^V < 150~\GeV$ analysis bin, therefore the aforementioned
nuisance parameters are decorrelated in this bin. This is so that the affect of
opposite sign deviations between data and the prediction do not cancel each
other out.
\input{07-systematic-errors/zjets-mbb-shapes-ptv-range}
The decorrelated parameters are named \texttt{SysZPtV\_BMin75\_L2} and
\texttt{SysMbb\_BMin75\_L2}. A summary of the $Z+$jets shape uncertainties is
found in table~\ref{tab:zjets-shapes}.
\input{07-systematic-errors/zjets-shape-table}

The fits to the ratios of the relevant quantities are performed on data in the
0--1~\TeV\ range. The functional form of the fit to the $ p_{\mathrm{T}}^V$
ratio is logarithmic, whereas the GSC $m_{bb}$ ratio is linear and it's value at
300~\GeV\ is levelled to the end of the range. This is to make sure that the
shape uncertainty is not overly influenced by events far from the Higgs mass.
Similarly the binning of the GSC $m_{bb}$ histograms is chosen to give the fit
more data points close the Higgs mass. The binning of the $p_{\mathrm{T}}^V$
distributions is chosen as to put more points where there are more available
events.

\clearpage
\newpage

\section{Systematic Uncertainties on \texorpdfstring{$t\bar{t}$}{tt} Events}
\label{sec:ttbar-systs}
Modelling of $t\bar{t}$ events differs between the leptonic channels of the
analysis. Like many of the differences between channels that have been discussed
already the reason for this ultimately boils down to there being two visible
charged leptons in the final state of the 2--lepton channel. Using these charged
leptons two regions can be defined in the 2--lepton channel, the same flavour
region which breaks down into the usual CR$_{\text{low}}$, SR and
CR$_{\text{high}}$ and the opposite flavour region known as the top $e\mu$
control region. These regions can be used to get a data driven estimate of the
background from top quark processes in the 2--lepton channel, described in
section~\ref{sec:ttbar_DD}. In the 0-- and 1--lepton channel Monte-Carlo
predictions and theoretical uncertainties are used instead. A summary of the
nuisance parameters that control the $t\bar{t}$ systematic uncertainties can be
found in table~\ref{tab:ttbar-systs}.
\input{07-systematic-errors/ttbar-table.tex}

\subsubsection{Nominal and alternative predictions}

The nominal Monte-Carlo prediction for the $t\bar{t}$ process is
\textsc{Powheg}~+~\textsc{Pythia}~8 with the \textsc{Powheg} NLO matrix element
(ME) generator~\cite{JHEP0709.2007.126,JHEP0411.2004.040} interfaced to
\textsc{Pythia}~8~\cite{Comp.Phys.Comm.191.159} using the A14
tune~\cite{ATL-PHYS-PUB-2014-021} to model parton showering, hadronisation,
the underlying event, and multiple parton interactions. The NNPDF3.0 (NLO) and
NNPDF2.3 parton distribution function sets are used in the ME calculation and
parton showering respectively~\cite{ATL-PHYS-PUB-2016-020}.

Details of the samples making up the nominal $t\bar{t}$ prediction can be found
in table~\ref{tab:ttbar-nominal} these include the high and low radiation
variations called  RadHi and RadLo respectively, the samples making up the
alternative prediction can be found in table~\ref{tab:ttbar-alternative}.
Samples are simulated with \textsc{Powheg}~+~\textsc{Herwig}~7 to vary the
parton shower model and \textsc{MadGraph}~5\textsc{\_aMC@NLO}+\textsc{Pythia}~8
to vary the matrix element calculation.

\subsubsection{Cross section}

The $t\bar{t}$ cross section relevant to this analysis is calculated for a top
quark mass of 172.5~\GeV\ and is $831.76^{+40}_{-46}$~pb. The calculation is  at
NNLO in QCD including re-summation of NNLL soft gluon terms with
\textsc{top++2.0}~\cite{Beneke2012695,Cacciari2012612,PhysRevLett.109.132001,NNLOcorr,NNLOcorrNLO,PhysRevLett.110.252004,Czakon:2011xx}.
The PDF and $\alpha_S$ uncertainties are calculated using the PDF4LHC
prescription ~\cite{Botje:2011sn} with the MSTW2008~68\% CL
NNLO~\cite{PDFLHC,alphasunc}, CT10
NNLO~\cite{PhysRevD.82.074024,PhysRevD.89.033009} and NNPDF2.3 5f
FFN~\cite{Ball:2012cx} PDF sets, added in quadrature to the scale uncertainty.

\subsubsection{Normalisation and Acceptance}

In all fit regions the normalisation of the $t\bar{t}$ background is kept as a
floating normalisation in 2-- and 3--jet categories independently. The nuisance
parameters relevant to this normalisation are \texttt{norm\_ttbar\_J2} and
\texttt{norm\_ttbar\_J3} respectively. In terms of acceptance uncertainties,
most of the potential migrations such as migration between analysis regions and
$p_{\mathrm{T}}^V$ bins are covered by the shape uncertainties. There is one nuisance
parameter, \texttt{SysTTbarNorm\_L0} which is implemented to cover migrations
between the 1-- and 0-- lepton channels, it is implemented in the combination of
the 2-- and 3--jet categories, applied to events in the 0--lepton channel and
has a prior calculated using the usual formula in
equation~\ref{eq:acceptance-dr}.

\subsubsection{Flavour Composition}

\begin{sloppypar}
  Flavour composition uncertainties are derived using ratios of flavour
  sub-components as for other backgrounds, however given that only the $bb$ and
  $bc$ sub-components are non-negligible so the remainder of sub-components are
  combined into $\text{Oth}~=~[bl, cc, cl, ll]$. Nuisance parameters are therefore
  implemented for $bc$ and Oth only with $bb$ on the bottom of the ratio as
  before. Given that neither the matrix element nor parton shower variation
  dominates the uncertainty heavily, nuisance parameters are implemented for each
  in turn, in total the nuisance parameters controlling flavour composition
  uncertainty are \texttt{SysTTbarbcMeACC}, \texttt{SysTTbarbcPSACC},
  \texttt{SysTTbarOthMeACC}, \texttt{SysTTbarOthPSACC}.
\end{sloppypar}

\subsubsection{Shape Uncertainties}

The $(N-1)$--dimensional parametrisation is used to estimate the shape
uncertainties on $t\bar{t}$ events. As in the $W+$jets the variable that is
factorised out of the multi-dimensional parametrisation is $p_{\mathrm{T}}^V$ which is
chosen for the same reasons as mentioned before. The shape systematic is not as
correlated over as many of the analysis bins as in the $W+$jets case. The
$p_{\mathrm{T}}^V$ shape uncertainty is correlated across 0-- and 1--lepton channels but
decorrelated in jet multiplicity as \texttt{SysTTbarPtV\_J2} and
\texttt{SysTTbarPtV\_J3} for the 2-- and 3--jet categories respectively. The
$p_{\mathrm{T}}^V$ shapes used are coming from the largest variation when considering all
samples which turns out to be the comparison of
\textsc{Powheg}~+~\textsc{Pythia}~8 and
\textsc{MadGraph}~5~\textsc{\_aMC@NLO}~+~\textsc{Pythia}~8. The BDTr shape
uncertainty is not clearly dominated by one comparison of generators and
therefore multiple nuisance parameters enter to encapsulate all the potential
sources of uncertainty. Differences due to the matrix element calculation are
correlated in leptonic channel but decorrelated in 2-- and 3--jet categories as
\texttt{SysBDTr\_ttbar\_ME\_J2} and \texttt{SysBDTr\_ttbar\_ME\_J3}
respectively. For the parton shower variation the uncertainties are correlated
in jet multiplicity but decorrelated in leptonic channel and also in the
1--lepton channel decorrelated in $p_{\mathrm{T}}^V$ bin, the nuisance parameters in
question are \texttt{SysBDTr\_ttbar\_PS\_L0},
\texttt{SysBDTr\_ttbar\_PS\_Bmin150\_L1}, and \texttt{SysBDTr\_ttbar\_PS}.

\subsection{Data Driven Estimation}
\label{sec:ttbar_DD}

As mentioned in section~\ref{sec:topemucr} the top $e\mu$ control region is
constructed and studied in the analysis in order to get a handle on top quark
backgrounds in the 2--lepton channel. Due to the flavour symmetry of the
$t\bar{t}$ and $$Wt$$ processes the shape of the top quark backgrounds in the top
$e\mu$ control region and the SR ought to be the same. In order to correct for
any differences in normalisation a scale factor $\alpha$ is derived as follows
\begin{align}
  N_{\text{top, data}}^{\text{SR}}
  =
  \frac{N_{\text{top, MC}}^{\text{SR}}}{N_{\text{top, MC}}^{\text{CR}}}
  \times
  N_{\text{top, data}}^{\text{ CR}}
  = \alpha \times N_{\text{top, data}}^{\text{ CR}}.
  \label{eq:TTbar_DD_ExtrapFac}
\end{align}
Once that factor is applied then the data from the top $e\mu$ control region can
be used as a template for top quark backgrounds in the SR.

Using the data driven estimate of the background eliminates the need to apply
the experimental systematics detailed in section~\ref{sec:experimental-systs}.
It also means we don't have to consider theoretical modelling uncertainties
associated with any Monte-Carlo generator. Note that systematic uncertainties on
the Monte-Carlo prediction used to derive the scale factor $\alpha$ cancel.

In order to check the validity of this approach a data versus prediction check
was carried out as part of series of wider checks~\cite{VHModellingNote2019}.
This check includes the data from the top $e\mu$ control region as the
prediction for both $t\bar{t}$ and single top events. The plots showing the
comparison can be found in figure~\ref{fig:ttbardd-mbb}.
\input{07-systematic-errors/ttbar-data-driven-check}
In order to see how the shape is modelled the scale factors controlling
normalisation of the other backgrounds have been taken from the previous
iteration of the analysis and applied, in this iteration there was no boundary
at 250~\GeV\ and therefore $p_{\mathrm{T}}^V$ bins 75--150~\GeV\ and
$\geq$150~\GeV\ are shown. The previous iteration also did not categorise events
into $\Delta R(b,b)$ based regions and therefore the sum of all the $\Delta
R(b,b)$ regions is shown. It can be seen in the ratio panels that the data and
prediction agree well and therefore the prediction from the top $e\mu$ control
region is considered to be accurate.

\clearpage
\newpage

\section{Systematic Uncertainties on Sub-Dominant Backgrounds}

This section details the systematic uncertainties on sub-dominant backgrounds.
None of these background processes dominate in any region of any channel of the
analysis and so they are not considered to have as large of an impact on the
analysis as the rest of the backgrounds. All of the uncertainties of
sub-dominant backgrounds are summarised in
table~\ref{tab:small_bkg_systematics}.
\input{07-systematic-errors/small-bkg-table.tex}
All of these systematic uncertainties are based on the comparison of different
Monte-Carlo predictions, apart from in the case of the multi-jet background
which is estimated using a data-driven process, details of the nominal
predictions for diboson are found in table~\ref{tab:diboson-nom}, and the
alternative predictions are found in table~\ref{tab:diboson-alt}. Details of all
single-top predictions are found in table~\ref{tab:st-nom-alt}.

\subsection{Multi-Jet}

Systematic uncertainties on the multi-jet background due to QCD are considered
only in the 1--lepton channel. This is due to heavy suppression of multi-jet
processes in the 0-- and 2--lepton channels. The uncertainties considered are on
the normalisation and shape effects only. For normalisation a nuisance parameter
is introduced for each category in jet multiplicity and each channel ($e$ or
$\mu$) of the multi-jet process, totalling four parameters. Two parameters
control the shape uncertainty which are associated to altering the isolation
criteria described in section~\ref{sec:lepton} and altering the scaling with
respect to the $W+$hf and top quark processes. 

\subsection{Single Top}

Systematic uncertainties on the single top process are applied to all regions of
the analysis and include parameters controlling the normalisation, acceptance
effects and shape effects. Normalisation is considered for the s-channel,
t-channel and $Wt$-channel single top decays individually. Acceptances in the
2--jet and 3--jet categories are considered for the t- and $Wt$-channels, where the
$Wt$-channel has a parameter for $bb$ and Oth flavour sub-components. Shape
uncertainties are considered for the $p_{\mathrm{T}}^V$ and $m_{bb}$ variables
for the t- and $Wt$-channels totally four parameters controlling the shapes.

\subsection{Diboson}

Systematic uncertainties on the diboson process are considered in all regions of
the analysis and include normalisation, acceptance and shape uncertainties.
There is one parameter controlling normalisation for each of the $WW$, $ZZ$ and
$WZ$ processes. A number of parameters controlling acceptance uncertainty due to
the underlying event and parton shower variations are introduced. There are two
for the $VZ$ process controlling the overall variation and the acceptance
between categories in jet multiplicity. There is one parameter for each of the
$WZ$ and $ZZ$ processes that controls the acceptance in the 0--lepton channel.
Additionally there are four parameters controlling acceptance uncertainty
originating from the chosen QCD strength parameter, there is one for acceptance
of $VZ$ events in the 2--jet category, one to control migration of $VZ$ events
between the 2-- and 3--jet categories, one that controls acceptance changes due
to the veto based on the total number of jets in any $VZ$ event, and one that
controls the acceptance of $gg$ initiated $ZZ$ events. Shape uncertainties are
considered, for variations due to the matrix element calculation and for
variations due to the underlying events and parton shower calculations, for the
$p_{\mathrm{T}}^V$ and $m_{bb}$ variables, totalling four parameters.

\section{Systematic Uncertainties on the Signal Process}

Predictions of the number of events expected due to the \VHbb\ signal process
are generated as follows. Processes initiated by a $q\bar{q}$ interaction are
simulated with
\textsc{Powheg}~+~\textsc{MiNLO}~+~\textsc{Pythia}~8~\cite{Luisoni2013,
  Sjostrand2008852} at NLO whereas processes initiated by a $gg$ interaction are
simulated with \textsc{Powheg}~+~\textsc{Pythia}~8 at LO. Both sets of samples
are have the AZNLO tune~\cite{Aad:2014xaa} applied with the NNPDF3.0
PDF~\cite{Ball:2014uwa} set being used. Alternative predictions are generated 
using variations of the internal parameters of the nominal generators and also
with \textsc{Powheg}~+~\textsc{MiNLO}~+~\textsc{Herwig}~7. Details of the
nominal and alternative predictions are found in
tables~\ref{tab:VHSMsignals-nom} and~\ref{tab:VHSMsignals-alt} respectively.

The cross section that samples are normalised to is the best theoretical
prediction available which is calculated at NNLO in QCD~\cite{Brein:2003wg,
  Brein:2011vx}, NLO in EW~\cite{Denner:2012sx} and includes higher order
contributions to the gluon-induced heavy quark loop entering into the \ZH\
calculation~\cite{Altenkamp:2012sx}.

A summary of the systematic uncertainties considered on the signal samples can
be found in table~\ref{tab:sig_systematics}.
\input{07-systematic-errors/signal-table.tex}
The majority of these uncertainties account for migration between STXS bins
(which are defined in $p_{\mathrm{T}}^V$) due to the QCD scale uncertainty.
Parameters are decorrelated for each STXS bin boundary (75, 150, 250, 400)~\GeV\
and for the $q\bar{q}$ and $gg$ initiated processes. Similarly there are
parameters that control for migration at the relevant $N_{\text{jet}} -
N_{H\text{jet}}$ boundaries that are decorrelated in the same way, these
boundaries come from the separations in the STXS scheme based on the number of
additional jets in an event beyond the two $b$-jets coming from the Higgs boson,
as per the previous STXS measurement~\cite{STXSpaper}. Additionally
there are parameters that control the uncertainty of the overall cross-section
of the $q\bar{q}$ and $gg$ processes due to the QCD scale uncertainty. More
parameters control uncertainties arising from the choice of PDF set which
control both shape and normalisation. A single parameter controls for the
uncertainty of the branching ratio of the $H \to b{\bar{b}}$ decay. Finally
several parameters control  $p_{\mathrm{T}}^V$ and $m_{bb}$ shape variations due
to NLO EW corrections and underlying event and parton shower calculations, with
the latter also being decorrelated based on sub-process.