\chapter{Fit Models}
\label{ch:fit-models}
\section{Profile Likelihood Fits}%
\label{sec:plf}
\subsection{Likelihood Function Definition}
\label{sec:lhoodDef}
The statistical analysis of the data uses a binned likelihood function which
maximum correspond to the best description of data. It is defined as the product
over all bins of the Poisson probability to observe $N^{\text{obs}}_b$ data
events given a prediction of
$N^{\text{exp}}_b(\mu,\mathbf{k},{\bm\theta})$ events in a certain bin
$i$:

\begin{equation}
  L(\mu,{\bm{k},\bm{\theta}}) =
  \prod_{i\in\,\text{bins}} \frac{\left( N_{i}^{\text{exp}}(\mu,{\bm{k,\theta}}) \right)^{N_{i}^{\text{data}}}}{N_{i}^{\text{data}}\,!}
  \cdot e^{-N_{i}^{\text{exp}}(\mu,{\bm{k,\theta}})}
\end{equation}

In this likelihood, the number of predicted events is made dependent on three
sets of parameters: the signal strength $\mu$, the scale factors
$\mathbf{k}=\left\{k_1, ...,k_j\right\}$, and the nuisance parameters
$\bm{\theta} = \left\{\theta_1,...,\theta_l\right\}$, as follows

\begin{equation}
  N_{i}^{\text{exp}}(\mu,\mathbf{k},\bm{\theta}) = \mu \cdot N_{i,\text{sig}}^{\text{exp}}(\bm{\theta}) + \sum_{b\in\,\text{bkg}} k_b\cdot N_{i,b}^{\text{exp}}(\bm{\theta})
\end{equation}
The parameter of interest $\mu=\sigma/\sigma_{\text{SM}}$ is common to all
channels and category and is the ratio between the measured and the expected
signal cross-sections.

As each scale factor does for it associated background component, the signal
strength scales the amount of signal linearly without any prior constraint, or
penalty in the likelihood function.


The nuisance parameters (NP) $\theta_i$ encode the dependence of the prediction
on systematic uncertainties into continuous parameters in the likelihood.

The prior knowledge on these parameters is reflected by a Gaussian penalty term
$\text{Gauss}(0\,|\,\theta_i,1)$ added to the likelihood for each NP, rending
displacement of these parameters depreciated.

The parameters $\theta_i$ are therefore expressed in standard deviation in the
following.

It results in a log-normal (normal) dependence of the predicted rates (shapes)
on the displayed parameter values.


The nominal fit result in terms of $\mu$ and $\sigma_{\mu}$ is obtained by
maximizing the likelihood function with respect to all parameters.  This is
referred to as the maximized log-likelihood value, MLL. The profile likelihood
ratio test statistic, $q_\mu$, is then constructed as follows:

\begin{equation}
  q_\mu = - 2\; \ln \left[ \mathcal{L} (\mu, \hat{\hat{\mathbf{k}}}, \hat{\hat{\bm\theta}}_{\mu})\, / \, \mathcal{L} (\hat{\mu}, \hat{\mathbf{k}}, \hat{\bm\theta}) \right]
\end{equation}
where $\hat{\mu}$ and $\hat{\theta}$ are the parameters that maximise the
likelihood (with the constraint $0 \leq \hat{\mu} \leq \mu$), and
$\hat{\hat{\theta}}_\mu$ are the nuisance parameter values that maximise the
likelihood for a given $\mu$. This test statistic is used to measure the
compatibility of the background-only model with the observed data, extracting
the local $p_0$ value, and, if no hint of a signal is found in this procedure,
for the derivation of exclusion intervals using the $CL_s$
method~\cite{Cowan:2010js,Read:2002hq}.


%\section{Fit Models}%
%\label{sec:fit-models}
\section{VH(b,b) Multi-Variate Discriminant Fit}%
\label{sec:mva-fit}
Main fit, primary result
\input{08-fit-models/fit-regions}
\section{Di-jet Mass Fit}%
\label{sec:mbb-fit}
Refer briefly to cut based cross-check, put results in appendix
\section{VZ Fit}%
\label{sec:mvadiboson-fit}
Refer briefly to diboson cross-check, put results in appendix

% \paragraph{Shape systematic Uncertainties post-processing} 

% In the current systematic model, independent normalization factors are employed
% for the main backgrounds (\ttbar, $\Wboson +$HF and $\Zboson +$HF) independently
% in $2$ and $3(+)$-jet, above or below $$p_T^V$ = 150~\GeV\$. Therefore, the
% extrapolation uncertainty is already accounted for at these boundaries. Most of
% the systematic uncertainties are computed in total $$p_T^V$$ phase space
% ($\geq75~\GeV$) in $2$-jet and $3$-jet events independently to avoid
% double-counting to the best possible. However, migrations at the $pTV=150~\GeV$
% are still possible, and differences in BDTr training and application setup or
% correlations could induce a small uncertainty componant accross jet bins. When
% visible, these effects are reduced with post-processing. The overall component
% of \ttbar ME, ttbar PS and \ttbar, $\Wboson +$HF, $\Zboson +$HF $p_T^V$
% uncertainties in the different jet bins, as well as in the bins
% $75~\GeV<$p_T^V$<150~\GeV$ and $$p_T^V$>150~\GeV$ is removed that way. \\


% Signal \mbb~shape uncertainty follow a similar but more refined treatment. In
% fact, the \VH signal is meant to have independent floating parameters in each
% STXS bin. Therefore, \VH~\mbb uncertainties are treated to have no overall
% impact in each jet and $p_T^V$ analysis bin, and act only as a shape on the
% discriminant, or as an extrapolation between signal and control regions. The
% same approach is adopted for di-boson \mbb uncertainties.

% \subsubsection{Smoothing of the Systematic Uncertainties}
% \label{sec:smooth}
% The uncertainties on reconstructed objects are propagated in the analysis in two
% different ways: by shifting weights, or by modifying the kinematic properties of
% the relevant objects and re-running the analysis chain. For flavour tagging,
% where a scale factor (SF) is used to correct efficiencies in the simulations to
% match those of data, the event weight is varied according to an upward
% (downward) shift of the SF and the change in the final distribution is noted as
% the +1 (-1) $\sigma$ uncertainties. For jet energy scale (JES) uncertainties,
% the jet energies are varied directly. Therefore events can migrate in and out of
% the analysis acceptance. Again the difference in the final discriminant is noted
% as the 1 $\sigma$ error. However, in the case of small variations and/or limited
% available MC statistics, the MC statistical uncertainty can make up a
% substantial part of this supposed systematic difference. Given that independent
% NP are introduce in the analysis to account for the MC statistical
% uncertainties, the inflation of systematic uncertainties due to limited
% statistic should be smoothed out.

% Two so-called ``smoothing'' algorithms are used to mitigate these effects. They
% have been developed for the Run 1 analysis and are based on the merging of
% consecutive bins in MC templates. Systematics templates are built as the ratios
% of varied to nominal MC templates. First, bins from one extremum to the next are
% merged until no local extrema remain in the BDT systematics template for the
% multi-variate analysis, or at most one extremum in the mbb systematics template
% for the cut-based analysis and well as the jet energy resolution systematics in
% the multi-variate analysis. This is an iterative process in which the merging
% performed at each step is chosen as the one for which the difference between
% merged and unmerged templates is smallest. Second, the bins resulting from this
% first algorithm are sequentially merged, starting from the upper end of the
% distribution, until the statistical uncertainty in each of the merged bins,
% calculated in the nominal template, is smaller than 5\%. In each of these merged
% bins, the nominal and systematically shifted contents are compared to give the
% $\pm1\sigma$ variation. This value is then used as the associated uncertainty
% for all the nominal bins in the corresponding merged bin.


% The smoothing procedure is applied to the uncertainties associated to:
% $e\gamma$, MET, muons, taus, jvt, jet, PRW and multi-jet modelling shapes. An
% example of the MET and JES systematics can be found in
% Figure~\ref{fig:Smoothing_Example}. The result of smoothing systematic
% uncertainties is checked to ensure that this is behaving as expected.

% % \begin{figure}[h]
% %   \centering
% %
%   \includegraphics[width=0.65\linewidth]{figures/stat/Smoothing/Region_BMax250_BMin150_Y6051_DSR_T2_L0_distmva_J2_VHSTXS_SysMET_SoftTrk_ResoPara.png}
% %
%   \includegraphics[width=0.65\linewidth]{figures/stat/Smoothing/Region_BMax250_BMin150_Y6051_DSR_T2_L0_distmBB_J3_VHSTXS_SysJET_CR_JET_EtaIntercalibration_Modelling.png}
% %      \caption{The variation in 0 lepton channel, 150\,\GeV$< p^{V}_{T}
% <$250\,\GeV region of the MET track-based soft term systematic in the
% multi-variate analysis (top) and the jet energy scale on eta-intercalibration
% systematic in the cut-based analysis (bottom). The dashed lines represent the
% systematic shape before smoothing and the solid lines represent the systematic
% shape after smoothing.}
% % \label{fig:Smoothing_Example}

% % \end{figure}


% \subsubsection{Pruning of the Systematic Uncertainties}
% \label{sec:smooth_prune}

% Several of the uncertainties described in Section~\ref{sec:npdefs} have a
% negligible effect on the distributions entering in the fit.  In addition,
% limited statistics in the MC nominal distributions can produce systematic
% templates with large fluctuations, introducing artificial variations in the fit.
% Therefore, following the Run 1 strategy, uncertainties are removed following a
% ``pruning'' procedure, which is carried out for each category/sub-channel in
% each region.

% Pruning is performed as follows: 
% \begin{itemize}
% \item Neglect the normalization uncertainty for a given sample in a region if either of the
%   following is true:
 
%  \begin{itemize}
%   \item the variation is less than 0.5\%
%   \item both up and down variations have the same sign
    
%  \end{itemize}

% \item Neglect the shape uncertainty for a given sample in a given region if the
%   following is true:
  
%  \begin{itemize}
%   \item not one single bin has a deviation over 0.5\% after the overall
%     normalization is removed
    
%   \item if only the up or the down variation is non-zero and passed the previous
%     pruning steps
    
%  \end{itemize}

% \item An additional pruning is made to remove systematic effects on small
%   samples in all regions. In any given region, that pruning is only applied to
%   samples contributing to less than $1\%$ of the total background and reads as
%   follow:
  
%  \begin{itemize}
%   \item in low sensitivity regions (no bin with $S/B>2\%$), normalization
%     effects below $5$ per mille of the total backgrounds and shapes varying no
%     bin by more than $5$ per mille of the total backgrounds are pruned
    
%   \item in regions where at least one bin has a signal contribution $>2\%$ of
%     the total background, shape and normalization effects are pruned if they
%     generate yield variations in these bins smaller than $2\%$ of the signal
%     yields in these bins
    
%  \end{itemize}
% \end{itemize}

% The list of pruned uncertainties is regularly checked to ensure that this is
% behaving as expected. The value of the threshold is also tested and compared to
% no-threshold for each stable iteration of the fit. This to ensure no
% over-pruning is made and the analysis sensitivity is not artificially increased.
